{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise 2.6 from the \"An Introduction to Counterfactual Regret Minimization\"\n",
    "# paper by Neller and Lanctot\n",
    "\n",
    "import numpy as np\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 5],\n",
       "       [0, 1, 4],\n",
       "       [0, 2, 3],\n",
       "       [0, 3, 2],\n",
       "       [0, 4, 1],\n",
       "       [0, 5, 0],\n",
       "       [1, 0, 4],\n",
       "       [1, 1, 3],\n",
       "       [1, 2, 2],\n",
       "       [1, 3, 1],\n",
       "       [1, 4, 0],\n",
       "       [2, 0, 3],\n",
       "       [2, 1, 2],\n",
       "       [2, 2, 1],\n",
       "       [2, 3, 0],\n",
       "       [3, 0, 2],\n",
       "       [3, 1, 1],\n",
       "       [3, 2, 0],\n",
       "       [4, 0, 1],\n",
       "       [4, 1, 0],\n",
       "       [5, 0, 0]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Colonel Blotto game definition\n",
    "\n",
    "S = 5\n",
    "N = 3\n",
    "\n",
    "ACTIONS = np.array([a for a in product(range(S + 1), repeat=N) if sum(a) == S])\n",
    "ACTION_INDICES = range(len(ACTIONS))\n",
    "ACTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utility(action_i_p_0, action_i_p_1):\n",
    "    wins_p_0 = np.sum(ACTIONS[action_i_p_0] > ACTIONS[action_i_p_1])\n",
    "    wins_p_1 = np.sum(ACTIONS[action_i_p_0] < ACTIONS[action_i_p_1])\n",
    "\n",
    "    if wins_p_0 > wins_p_1:\n",
    "        return 1, -1\n",
    "    elif wins_p_0 < wins_p_1:\n",
    "        return -1, 1\n",
    "    else:\n",
    "        return 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 5] - player0 p=0.0000 - player1 p=0.0000\n",
      "[0 1 4] - player0 p=0.0001 - player1 p=0.0001\n",
      "[0 2 3] - player0 p=0.0870 - player1 p=0.1066\n",
      "[0 3 2] - player0 p=0.1560 - player1 p=0.1103\n",
      "[0 4 1] - player0 p=0.0004 - player1 p=0.0000\n",
      "[0 5 0] - player0 p=0.0000 - player1 p=0.0000\n",
      "[1 0 4] - player0 p=0.0001 - player1 p=0.0001\n",
      "[1 1 3] - player0 p=0.1136 - player1 p=0.1162\n",
      "[1 2 2] - player0 p=0.0001 - player1 p=0.0005\n",
      "[1 3 1] - player0 p=0.0714 - player1 p=0.1087\n",
      "[1 4 0] - player0 p=0.0003 - player1 p=0.0002\n",
      "[2 0 3] - player0 p=0.1174 - player1 p=0.1116\n",
      "[2 1 2] - player0 p=0.0001 - player1 p=0.0001\n",
      "[2 2 1] - player0 p=0.0001 - player1 p=0.0002\n",
      "[2 3 0] - player0 p=0.1346 - player1 p=0.1113\n",
      "[3 0 2] - player0 p=0.0849 - player1 p=0.1045\n",
      "[3 1 1] - player0 p=0.1277 - player1 p=0.1111\n",
      "[3 2 0] - player0 p=0.1060 - player1 p=0.1182\n",
      "[4 0 1] - player0 p=0.0000 - player1 p=0.0001\n",
      "[4 1 0] - player0 p=0.0002 - player1 p=0.0002\n",
      "[5 0 0] - player0 p=0.0000 - player1 p=0.0000\n"
     ]
    }
   ],
   "source": [
    "# regret matching\n",
    "\n",
    "NUM_ITERATIONS = 20000\n",
    "\n",
    "regrets_sum_p_0 = np.zeros(len(ACTIONS))\n",
    "regrets_sum_p_1 = np.zeros(len(ACTIONS))\n",
    "\n",
    "strategy_sum_p_0 = np.zeros(len(ACTIONS))\n",
    "strategy_sum_p_1 = np.zeros(len(ACTIONS))\n",
    "\n",
    "def regret_matching_strategy(regrets_sum):\n",
    "    strategy = regrets_sum.copy()\n",
    "    strategy[strategy < 0] = 0\n",
    "    normalizing_sum = np.sum(strategy)\n",
    "    if normalizing_sum == 0:\n",
    "        return np.full(strategy.shape, 1.0 / len(strategy))\n",
    "    return strategy / normalizing_sum\n",
    "\n",
    "\n",
    "def current_regret(action_i_me, action_i_opponent):\n",
    "    regret = np.zeros(len(ACTIONS))\n",
    "    actual_utility = utility(action_i_me, action_i_opponent)[0]\n",
    "\n",
    "    for action_i in range(len(ACTIONS)):\n",
    "        regret[action_i] = utility(action_i, action_i_opponent)[0] - actual_utility\n",
    "    return regret\n",
    "\n",
    "\n",
    "def sample_action(strategy):\n",
    "    return np.random.choice(ACTION_INDICES, size=1, p=strategy)[0]\n",
    "\n",
    "\n",
    "for _ in range(NUM_ITERATIONS):\n",
    "    strategy_p_0 = regret_matching_strategy(regrets_sum_p_0)\n",
    "    strategy_p_1 = regret_matching_strategy(regrets_sum_p_1)\n",
    "\n",
    "    strategy_sum_p_0 += strategy_p_0\n",
    "    strategy_sum_p_1 += strategy_p_1\n",
    "\n",
    "    action_p_0 = sample_action(strategy_p_0)\n",
    "    action_p_1 = sample_action(strategy_p_1)\n",
    "\n",
    "    regrets_sum_p_0 += current_regret(action_p_0, action_p_1)\n",
    "    regrets_sum_p_1 += current_regret(action_p_1, action_p_0)\n",
    "\n",
    "strategy_avg_p_0 = strategy_sum_p_0 / NUM_ITERATIONS\n",
    "strategy_avg_p_1 = strategy_sum_p_1 / NUM_ITERATIONS\n",
    "\n",
    "np.testing.assert_almost_equal(np.sum(strategy_avg_p_0), 1.0)\n",
    "np.testing.assert_almost_equal(np.sum(strategy_avg_p_1), 1.0)\n",
    "\n",
    "\n",
    "def print_strategies():\n",
    "    for action, p0, p1 in zip(ACTIONS, strategy_avg_p_0, strategy_avg_p_1):\n",
    "        print(\"{} - player0 p={:.4f} - player1 p={:.4f}\".format(action, p0, p1))\n",
    "\n",
    "\n",
    "print_strategies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the strategies\n",
    "\n",
    "NUM_GAMES = 10000\n",
    "\n",
    "\n",
    "def play_games(strategy_p_0, strategy_p_1, num_games=NUM_GAMES):\n",
    "    wins_p_0 = 0\n",
    "    wins_p_1 = 0\n",
    "    draws = 0\n",
    "\n",
    "    for _ in range(NUM_GAMES):\n",
    "        action_p_0 = sample_action(strategy_p_0)\n",
    "        action_p_1 = sample_action(strategy_p_1)\n",
    "        utility_p_0 = utility(action_p_0, action_p_1)[0]\n",
    "        if utility_p_0 == 1:\n",
    "            wins_p_0 += 1\n",
    "        elif utility_p_0 == -1:\n",
    "            wins_p_1 += 1\n",
    "        else:\n",
    "            draws += 1\n",
    "\n",
    "    print(\"Player 0 wins: {:d} ({:.4f})\".format(wins_p_0, wins_p_0 / num_games))\n",
    "    print(\"Player 1 wins: {:d} ({:.4f})\".format(wins_p_1, wins_p_1 / num_games))\n",
    "    print(\"Draws:         {:d} ({:.4f})\".format(draws, draws / num_games))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both players trained\n",
      "Player 0 wins: 2177 (0.2177)\n",
      "Player 1 wins: 2227 (0.2227)\n",
      "Draws:         5596 (0.5596)\n"
     ]
    }
   ],
   "source": [
    "print(\"Both players trained\")\n",
    "play_games(strategy_avg_p_0, strategy_avg_p_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player 0 trained, player 1 uniform\n",
      "Player 0 wins: 3037 (0.3037)\n",
      "Player 1 wins: 1800 (0.1800)\n",
      "Draws:         5163 (0.5163)\n"
     ]
    }
   ],
   "source": [
    "print(\"Player 0 trained, player 1 uniform\")\n",
    "strategy_uniform = np.ones(len(ACTIONS)) / len(ACTIONS)\n",
    "play_games(strategy_avg_p_0, strategy_uniform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player 0 trained, player 1 always selecting action [0 0 5]\n",
      "Player 0 wins: 5522 (0.5522)\n",
      "Player 1 wins: 0 (0.0000)\n",
      "Draws:         4478 (0.4478)\n",
      "\n",
      "Player 0 trained, player 1 always selecting action [0 1 4]\n",
      "Player 0 wins: 3104 (0.3104)\n",
      "Player 1 wins: 2002 (0.2002)\n",
      "Draws:         4894 (0.4894)\n",
      "\n",
      "Player 0 trained, player 1 always selecting action [0 2 3]\n",
      "Player 0 wins: 2030 (0.2030)\n",
      "Player 1 wins: 2138 (0.2138)\n",
      "Draws:         5832 (0.5832)\n",
      "\n",
      "Player 0 trained, player 1 always selecting action [0 3 2]\n",
      "Player 0 wins: 2380 (0.2380)\n",
      "Player 1 wins: 2327 (0.2327)\n",
      "Draws:         5293 (0.5293)\n",
      "\n",
      "Player 0 trained, player 1 always selecting action [0 4 1]\n",
      "Player 0 wins: 3177 (0.3177)\n",
      "Player 1 wins: 2422 (0.2422)\n",
      "Draws:         4401 (0.4401)\n",
      "\n",
      "Player 0 trained, player 1 always selecting action [0 5 0]\n",
      "Player 0 wins: 5200 (0.5200)\n",
      "Player 1 wins: 0 (0.0000)\n",
      "Draws:         4800 (0.4800)\n",
      "\n",
      "Player 0 trained, player 1 always selecting action [1 0 4]\n",
      "Player 0 wins: 3736 (0.3736)\n",
      "Player 1 wins: 2413 (0.2413)\n",
      "Draws:         3851 (0.3851)\n",
      "\n",
      "Player 0 trained, player 1 always selecting action [1 1 3]\n",
      "Player 0 wins: 2422 (0.2422)\n",
      "Player 1 wins: 2422 (0.2422)\n",
      "Draws:         5156 (0.5156)\n",
      "\n",
      "Player 0 trained, player 1 always selecting action [1 2 2]\n",
      "Player 0 wins: 2515 (0.2515)\n",
      "Player 1 wins: 1255 (0.1255)\n",
      "Draws:         6230 (0.6230)\n",
      "\n",
      "Player 0 trained, player 1 always selecting action [1 3 1]\n",
      "Player 0 wins: 2018 (0.2018)\n",
      "Player 1 wins: 1889 (0.1889)\n",
      "Draws:         6093 (0.6093)\n",
      "\n",
      "Player 0 trained, player 1 always selecting action [1 4 0]\n",
      "Player 0 wins: 3332 (0.3332)\n",
      "Player 1 wins: 2405 (0.2405)\n",
      "Draws:         4263 (0.4263)\n",
      "\n",
      "Player 0 trained, player 1 always selecting action [2 0 3]\n",
      "Player 0 wins: 2399 (0.2399)\n",
      "Player 1 wins: 2217 (0.2217)\n",
      "Draws:         5384 (0.5384)\n",
      "\n",
      "Player 0 trained, player 1 always selecting action [2 1 2]\n",
      "Player 0 wins: 1934 (0.1934)\n",
      "Player 1 wins: 672 (0.0672)\n",
      "Draws:         7394 (0.7394)\n",
      "\n",
      "Player 0 trained, player 1 always selecting action [2 2 1]\n",
      "Player 0 wins: 2410 (0.2410)\n",
      "Player 1 wins: 1134 (0.1134)\n",
      "Draws:         6456 (0.6456)\n",
      "\n",
      "Player 0 trained, player 1 always selecting action [2 3 0]\n",
      "Player 0 wins: 2133 (0.2133)\n",
      "Player 1 wins: 2000 (0.2000)\n",
      "Draws:         5867 (0.5867)\n",
      "\n",
      "Player 0 trained, player 1 always selecting action [3 0 2]\n",
      "Player 0 wins: 1989 (0.1989)\n",
      "Player 1 wins: 2069 (0.2069)\n",
      "Draws:         5942 (0.5942)\n",
      "\n",
      "Player 0 trained, player 1 always selecting action [3 1 1]\n",
      "Player 0 wins: 2395 (0.2395)\n",
      "Player 1 wins: 2524 (0.2524)\n",
      "Draws:         5081 (0.5081)\n",
      "\n",
      "Player 0 trained, player 1 always selecting action [3 2 0]\n",
      "Player 0 wins: 2206 (0.2206)\n",
      "Player 1 wins: 2348 (0.2348)\n",
      "Draws:         5446 (0.5446)\n",
      "\n",
      "Player 0 trained, player 1 always selecting action [4 0 1]\n",
      "Player 0 wins: 3585 (0.3585)\n",
      "Player 1 wins: 2400 (0.2400)\n",
      "Draws:         4015 (0.4015)\n",
      "\n",
      "Player 0 trained, player 1 always selecting action [4 1 0]\n",
      "Player 0 wins: 3200 (0.3200)\n",
      "Player 1 wins: 1973 (0.1973)\n",
      "Draws:         4827 (0.4827)\n",
      "\n",
      "Player 0 trained, player 1 always selecting action [5 0 0]\n",
      "Player 0 wins: 5552 (0.5552)\n",
      "Player 1 wins: 0 (0.0000)\n",
      "Draws:         4448 (0.4448)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for action_i in range(len(ACTIONS)):\n",
    "    strategy_pure = np.zeros(len(ACTIONS))\n",
    "    strategy_pure[action_i] = 1.0\n",
    "    print(\"Player 0 trained, player 1 always selecting action {}\".format(ACTIONS[action_i]))\n",
    "    play_games(strategy_avg_p_0, strategy_pure)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
